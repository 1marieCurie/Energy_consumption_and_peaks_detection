{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eef5a85",
   "metadata": {},
   "source": [
    "# Prédiction de Consommation Énergétique en kWh et Détection des Heures de Pic\n",
    "\n",
    "## Résumé Exécutif\n",
    "\n",
    "Ce projet vise à développer des modèles d'apprentissage automatique pour **prédire la consommation électrique totale (en kWh)** des bâtiments résidentiels marocains et **détecter les heures de pic énergétique**. En combinant techniques de régression et classification, nous cherchons à fournir des outils permettant une meilleure gestion de l'énergie, l'optimisation des tarifs d'électricité et la planification des ressources énergétiques.\n",
    "\n",
    "\n",
    "\n",
    "## Objectifs Principaux\n",
    "\n",
    "### 1. **Prédiction de Consommation Énergétique**\n",
    "\n",
    "Développer un modèle de régression capable de **prédire la consommation électrique totale (y)** en kilowatts-heures (kWh) pour une période donnée (minute, heure, ou jour).\n",
    "\n",
    "**Sous-objectifs :**\n",
    "- Modéliser les patterns temporels de consommation (houraire, quotidien, saisonnier)\n",
    "- Capturer les relations non-linéaires entre features temporelles et consommation\n",
    "- Intégrer les métadonnées des bâtiments (nombre d'occupants, type de logement)\n",
    "- Évaluer la performance du modèle via RMSE, MAE et MAPE\n",
    "\n",
    "### 2. **Détection des Heures de Pic**\n",
    "\n",
    "Identifier les **périodes d'anomalie ou de surcharge énergétique** où la consommation dépasse les seuils normaux.\n",
    "\n",
    "**Sous-objectifs :**\n",
    "- Classifier les heures comme \"pic\" ou \"normal\" via approche seuil et/ou machine learning\n",
    "- Détecter les appareils à forte consommation (four, chauffe-eau, climatisation)\n",
    "- Fournir une alerte précoce sur les heures critiques\n",
    "- Optimiser les tarifs et la planification du réseau électrique\n",
    "\n",
    "---\n",
    "\n",
    "## Contexte et Enjeux\n",
    "\n",
    "### Pourquoi ce projet ?\n",
    "\n",
    "La gestion efficace de l'énergie est devenue un enjeu majeur :\n",
    "\n",
    "**Consommation croissante** : La demande d'électricité augmente rapidement, notamment dans les pays en développement comme le Maroc.\n",
    "\n",
    "**Surcharge réseau** : Les pics de consommation causent une instabilité du réseau et augmentent les coûts opérationnels.\n",
    "\n",
    "**Coûts énergétiques** : Les ménages et entreprises cherchent à optimiser leurs dépenses et réduire leur empreinte énergétique.\n",
    "\n",
    "**Transition énergétique** : L'intégration croissante des énergies renouvelables (solaire, éolienne) rend la prédiction énergétique critique pour équilibrer l'offre et la demande.\n",
    "\n",
    "### Applications Pratiques\n",
    "\n",
    "Ce projet peut bénéficier à plusieurs acteurs :\n",
    "\n",
    "- **Distributeurs d'électricité** : optimiser la gestion du réseau et prévenir les surcharges\n",
    "- **Consommateurs** : identifier les périodes de forte consommation et ajuster leur usage\n",
    "- **Entreprises** : planifier leur consommation et réduire leurs factures énergétiques\n",
    "- **Planificateurs énergétiques** : estimer les besoins futurs et dimensionner les installations\n",
    "\n",
    "\n",
    "### Livrables du Projet\n",
    "\n",
    "**Modèles entraînés** : fichiers de poids/coefficients réutilisables\n",
    "\n",
    "**Rapports de performance** : comparaison des modèles (baseline vs avancé)\n",
    "\n",
    "**Analyse exploratoire** : visualisations des patterns temporels et pics\n",
    "\n",
    "**Pipeline complet** : code Python réutilisable pour intégration en production\n",
    "\n",
    "**Documentation technique** : guide d'utilisation et interprétation des résultats\n",
    "\n",
    "\n",
    "\n",
    "## Planification Générale\n",
    "\n",
    "| Phase | Durée | Tâches |\n",
    "|-------|-------|--------|\n",
    "| **1. Exploration & Nettoyage** | 1-2 semaines | Charger MORED, EDA, gestion des anomalies |\n",
    "| **2. Feature Engineering** | 2-3 semaines | Créer features temporelles, statistiques, cycliques |\n",
    "| **3. Régression** | 3-4 semaines | Entraîner modèles, tuning hyperparamètres, évaluation |\n",
    "| **4. Classification (Pics)** | 2-3 semaines | Définir labels, entraîner classifieurs, optimisation |\n",
    "| **5. Comparaison & Documentation** | 1-2 semaines | Benchmarking, visualisations finales, rapport |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0df292",
   "metadata": {},
   "source": [
    "# Introduction au Dataset MORED\n",
    "\n",
    "## Vue d'ensemble\n",
    "\n",
    "**MORED** est le premier dataset africain ouvert de consommation électrique de bâtiments. Il contient des données labellisées de consommation globale (WP) et par appareil (IL), ainsi que des signatures de charges individuelles provenant de plusieurs ménages et charges marocains.\n",
    "\n",
    "Le dataset a été mis à disposition par le TICLab de l'Université Internationale de Rabat (UIR). La collecte des données s'est déroulée dans le cadre du projet de recherche **PVBuild**, coordonné par le Prof. Mounir Ghogho et financé par l'Agence des États-Unis pour le développement international (USAID).\n",
    "\n",
    "---\n",
    "\n",
    "## Objectif et Motivation\n",
    "\n",
    "MORED est destiné à la recherche dans les domaines liés à l'énergie, tels que la désagrégation de charge (NILM - Non-Intrusive Load Monitoring) et la prévision énergétique.\n",
    "\n",
    "Le dataset vise à progresser dans le domaine de la désagrégation énergétique en offrant plus de données et en utilisant les avancées récentes du secteur. L'importance de ce dataset réside notamment dans sa nature **pionnière** : c'est le premier dataset de consommation électrique collecté auprès de ménages marocains.\n",
    "\n",
    "---\n",
    "\n",
    "## Structure et Composantes Principales\n",
    "\n",
    "MORED fournit trois types principaux de composantes de données :\n",
    "\n",
    "### 1. **Whole Premises (WP) - Consommation Globale**\n",
    "\n",
    "Les mesures WP reflètent la consommation électrique totale des bâtiments. Elles sont acquises au niveau du compteur principal et caractérisent l'ensemble du bâtiment/ménage.\n",
    "\n",
    "**Caractéristiques des données WP :**\n",
    "- Acquisitionnées à des taux bas (1/5 ou 1/10 échantillons/s) depuis 12 ménages\n",
    "- Puissance instantanée en watts (W)\n",
    "- Tension RMS (Vrms) en volts\n",
    "- Mesures temporelles avec granularité fine (10 à 20 secondes)\n",
    "\n",
    "Il existe bien autres types de datasets (Individual Load Ground-Truth (ILGT) ou Individual Load Signatures (ILS)) qui ne sont pas utilisable dans le cas de notre projet de consommation énergétique des bâtiment de manière globale.\n",
    "\n",
    "---\n",
    "\n",
    "## Couverture Géographique et Diversité\n",
    "\n",
    "Le dataset couvre plusieurs villes marocaines :\n",
    "- **Salé** (ménages en zones défavorisées)\n",
    "- **Tétuan** (ménages en zones aisées et défavorisées)\n",
    "- **Rabat** (ménages en zones aisées)\n",
    "\n",
    "\n",
    "## Caractéristiques des Données Acquises\n",
    "\n",
    "### Métadonnées des Ménages\n",
    "\n",
    "Pour chaque ménage ou bâtiment, le dataset inclut :\n",
    "\n",
    "| Métadonnée | Description |\n",
    "|-----------|------------|\n",
    "| **Nombre d'occupants** | Varie de 1 à 8 personnes |\n",
    "| **Composition démographique** | Nombre d'enfants, adultes, personnes âgées (>65 ans) |\n",
    "| **Surface habitable** | De 50 à 103 m² |\n",
    "| **Nombre de pièces** | Généralement 3 à 6 pièces |\n",
    "| **Nombre d'étages** | De 1 à 3 étages |\n",
    "| **Statut de propriété** | Location (R) ou Propriété (B) |\n",
    "| **Appareils monitorés** | De 2 à 8 charges individuelles selon le ménage |\n",
    "\n",
    "### Taux d'Échantillonnage\n",
    "\n",
    "- **WP** : 1/5 s (0,2 s) ou 1/10 s (0,1 s) — environ 5 à 10 mesures par seconde\n",
    "\n",
    "### Durée des Acquisitions\n",
    "\n",
    "- **WP** : de 14 à 90 jours par ménage\n",
    "- Couverture temporelle suffisante pour capturer patterns quotidiens, hebdomadaires et saisonniers\n",
    "\n",
    "\n",
    "Le dataset MORED est particulièrement adapté à notre projet pour :\n",
    "\n",
    "1. **Détection d'anomalies** : identification des pics de consommation et des heures critiques\n",
    "2. **Prédire les quantités d'énergie** : utilisation pour estimer les besoins énergétiques\n",
    "\n",
    "\n",
    "### Références\n",
    "\n",
    "Le dataset est accessible via :\n",
    "- [Site officiel MORED](https://moredataset.github.io/MORED/)\n",
    "- [Dépôt GitHub MOREDataset/MORED](https://github.com/MOREDataset/MORED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3495c2b",
   "metadata": {},
   "source": [
    "## Exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2550227f",
   "metadata": {},
   "source": [
    "On va expliquer le processus avec un seul fichier APT et son fichier associé de metadonné, puis on applique tout sur le reste des fichiers de données APTx sous /premises_data ainsi les fichiers de metadonnées associés sous /metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c21fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "N_FILES = 12\n",
    "SEC_PER_SAMPLE = 10\n",
    "\n",
    "df = pd.read_csv('premises_data/APT1.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea67049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6aa6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertit la colonne timestamp en type datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y %H:%M:%S', errors='coerce') #Si une valeur ne peut pas être parsée => NAT\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039dc2e",
   "metadata": {},
   "source": [
    "#### Remarques\n",
    "- la colonne 'real_power' est une puissance en watt.\n",
    "- Dans ce projet, nous avons besoin de l'energie **instantannée** en (Kw/h), c-à-d:   **Ei =Pi×Δt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97073b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['kWh_sample'] = df['real_power'] * (SEC_PER_SAMPLE / 3600.0) / 1000.0\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285e0fe4",
   "metadata": {},
   "source": [
    "Traitons maintenant la redondance des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ccca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupby('timestamp').agg({'kWh_sample':'sum'}).reset_index()  #\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9385a63b",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, nous allons remédier aux timestamps monquants:  \n",
    "a. Nous allons créé les timestamps du *start_time* jusqu'à *end_time* avec un intervalle de 10s.  \n",
    "b. Réindexer la df par la colonne *timestamp*.  \n",
    "c. Aligner le DataFrame sur *full_index*, en ajoutant des lignes pour tous les timestamps manquants.  \n",
    "1) Si df n’avait pas de donnée pour un timestamp dans full_index, la ligne est ajoutée avec NaN.  \n",
    "2) Si df avait déjà la donnée, elle reste inchangée.\n",
    "\n",
    "d. Renommer la colonne de l'indice par *timestamp*.  \n",
    "e. Remettre l’index en colonne normale, donc df['timestamp'] redevient une colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7304d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = df['timestamp'].min()\n",
    "end_time = df['timestamp'].max()\n",
    "full_index = pd.date_range(start=df['timestamp'].min(), end=df['timestamp'].max(), freq=f'{SEC_PER_SAMPLE}s')\n",
    "df = df.set_index('timestamp').reindex(full_index).rename_axis('timestamp').reset_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc343065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S'assurer que la difference entre les timestamps est exactement 10s\n",
    "df['delta'] = df['timestamp'].diff().dt.total_seconds()\n",
    "df['delta'].value_counts()\n",
    "df = df.drop('delta', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94507978",
   "metadata": {},
   "source": [
    "Calculons la consomtion d'energie par heure (Kw/h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = (\n",
    "    df.resample('1h', on='timestamp')\n",
    "      .sum()[['kWh_sample']]\n",
    "      .rename(columns={'kWh_sample': 'y_kWh'})\n",
    "      .reset_index()\n",
    ")\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeff0186",
   "metadata": {},
   "source": [
    "Les nouvelles infos sur la df => nous avons 992 lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4959b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d5e31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation de la cible y_Kwh en fonction de timestamp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df_hour['timestamp'], df_hour['y_kWh'], marker='o', linestyle='-')\n",
    "plt.title(\"Variation de la consommation horaire (y_kWh) dans le temps\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"y_kWh\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804f107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# importer le fichier building correspondant à ce batiment\n",
    "# Ouvrir le fichier YAML\n",
    "with open(\"metadata/building1.yaml\", \"r\") as file:\n",
    "    building_meta = yaml.safe_load(file)\n",
    "building_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eccdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour['n_occupants'] = building_meta.get('n_occupants', 0)\n",
    "\n",
    "building_type = building_meta.get('building_type', 'flat')\n",
    "df_hour['building_type'] = 1 if building_type == 'flat' else 0\n",
    "\n",
    "ownership = building_meta.get('ownership', 'rented')\n",
    "mapping_own = {'rented': 0, 'owned': 1, 'bought': 2}\n",
    "df_hour['ownership'] = mapping_own.get(ownership, 0)\n",
    "\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf7b8d4",
   "metadata": {},
   "source": [
    "#### Ajout de données temporels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749c5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour['hour'] = df_hour['timestamp'].dt.hour\n",
    "df_hour['year'] = df_hour['timestamp'].dt.year\n",
    "df_hour['day_of_week'] = df_hour['timestamp'].dt.dayofweek\n",
    "df_hour['is_weekend'] = df_hour['day_of_week'].isin([5,6]).astype(int)\n",
    "df_hour['month'] = df_hour['timestamp'].dt.month\n",
    "\n",
    "nouvel_ordre = [\n",
    "    'timestamp', \n",
    "    'year',         \n",
    "    'month', \n",
    "    'day_of_week', \n",
    "    'hour', \n",
    "    'is_weekend',  \n",
    "    'n_occupants', \n",
    "    'building_type', \n",
    "    'ownership',\n",
    "    'y_kWh',\n",
    "]\n",
    "\n",
    "# Réorganisation\n",
    "df_hour = df_hour[nouvel_ordre]\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fa7546",
   "metadata": {},
   "source": [
    "Dans la cellule suivante nous allons appliquer une transformation cyclique grâce aux *sin* et *cos* pour hour, day_of_week, month ; permet aux modèles ML de capturer la circularité (ex. **23h proche de 0h**) sans ordonner linéairement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c6483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour['hour_sin'] = np.sin(2*np.pi*df_hour['hour']/24)\n",
    "df_hour['hour_cos'] = np.cos(2*np.pi*df_hour['hour']/24)\n",
    "df_hour['dow_sin'] = np.sin(2*np.pi*df_hour['day_of_week']/7)\n",
    "df_hour['dow_cos'] = np.cos(2*np.pi*df_hour['day_of_week']/7)\n",
    "df_hour['month_sin'] = np.sin(2*np.pi*df_hour['month']/12)\n",
    "df_hour['month_cos'] = np.cos(2*np.pi*df_hour['month']/12)\n",
    "\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992214c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouvel_ordre = [\n",
    "    'timestamp', \n",
    "    'year',         \n",
    "    'month', \n",
    "    'day_of_week', \n",
    "    'hour', \n",
    "    'is_weekend',  \n",
    "    'n_occupants', \n",
    "    'building_type', \n",
    "    'ownership',\n",
    "    'is_peak',\n",
    "    'hour_sin',\n",
    "    'hour_cos',\n",
    "    'dow_sin',\n",
    "    'dow_cos',\n",
    "    'month_sin',\n",
    "    'month_cos',\n",
    "    'y_kWh'\n",
    "]\n",
    "df_hour = df_hour[nouvel_ordre]\n",
    "df_hour.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edad0f3",
   "metadata": {},
   "source": [
    "- Il ne nous reste, alors, que d'enregistrer le résultat final du traitement dans un dossier appart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7cfddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_file = os.path.join(OUTPUT_FILLED_DIR, f\"APT{i}_features_filled.csv\")\n",
    "\n",
    "fichier_final = 'preprocessing_results/APT1_processed.csv'\n",
    "df_hour.to_csv(fichier_final, index=False)\n",
    "print(f\"{fichier_final} créé avec {df_hour['y_kWh'].isna().sum()} NaN restants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6669169",
   "metadata": {},
   "source": [
    "#### Vérification d'existance de valeur y_kWh = 0 (problème de collecte d'information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af08b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('preprocessing_results/APT1_processed.csv')\n",
    "nb_zeros = (df[\"y_kWh\"] == 0).sum()\n",
    "print(\"Nombre de lignes où y_kWh = 0 :\", nb_zeros)\n",
    "print(\"*\"*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c49540e",
   "metadata": {},
   "source": [
    "Dans la cellule suivante, nous définissons une fonction destinée à compléter les valeurs nulles de y_kWh selon les règles suivantes :\n",
    "1) Toute valeur y_kWh = 0 est considérée comme un gap.\n",
    "2) Si ce gap est encadré par deux valeurs non nulles, une interpolation linéaire est appliquée.\n",
    "3) Dans les autres cas, la valeur est remplacée par la consommation énergétique moyenne observée à la même heure pour le bâtiment concerné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa8af67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fix_energy_gaps(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y-%m-%d %H:%M:%S', errors='coerce') #Si une valeur ne peut pas être parsée => NAT\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "    # Identifier les gaps = valeurs Nan ou égales à 0\n",
    "    df[\"is_gap\"] = df[\"y_kWh\"].isna() | (df[\"y_kWh\"] == 0)\n",
    "\n",
    "    # Copie de travail\n",
    "    df[\"y_kWh_fixed\"] = df[\"y_kWh\"].replace(0, np.nan)\n",
    "\n",
    "    # Interpolation pour valeurs réellement encadrées\n",
    "    df[\"y_kWh_interpolated\"] = df[\"y_kWh_fixed\"].interpolate(\n",
    "        method=\"linear\",\n",
    "        limit_direction=\"both\"\n",
    "    )\n",
    "\n",
    "    # Détection : interpolation valide SEULEMENT si la valeur interpolée est entre deux non-null\n",
    "    mask_start_valid = df[\"y_kWh_fixed\"].notna() | df[\"y_kWh_fixed\"].notna().shift(-1)\n",
    "    mask_end_valid   = df[\"y_kWh_fixed\"].notna() | df[\"y_kWh_fixed\"].notna().shift(1)\n",
    "\n",
    "    # Gap réellement encadré par 2 valeurs non-nulles\n",
    "    df[\"is_encadré\"] = mask_start_valid & mask_end_valid & df[\"is_gap\"]\n",
    "\n",
    "    # On ne met l’interpolation QUE pour les gaps encadrés\n",
    "    df.loc[df[\"is_encadré\"], \"y_kWh_fixed\"] = df.loc[df[\"is_encadré\"], \"y_kWh_interpolated\"]\n",
    "\n",
    "    # Pour les autres (début/fin série ou blocs de gaps)\n",
    "    df[\"problematic_gap\"] = df[\"is_gap\"] & (~df[\"is_encadré\"])\n",
    "\n",
    "    # === Solution proposée pour ces gaps ===\n",
    "    # OPTION 1 : remplacer par moyenne horaire historique\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    \n",
    "    # Calculer moyenne horaire SANS les zéros (car ce sont des gaps)\n",
    "    hourly_mean = df.loc[df[\"y_kWh\"] > 0].groupby(\"hour\")[\"y_kWh\"].mean()\n",
    "\n",
    "    # Remplacer les gaps problématiques\n",
    "    df.loc[df[\"problematic_gap\"], \"y_kWh_fixed\"] = \\\n",
    "    df.loc[df[\"problematic_gap\"], \"hour\"].map(hourly_mean)\n",
    "\n",
    "    # S'il reste des NaN à cause d'une heure totalement vide, fallback sur moyenne globale\n",
    "    global_mean = df.loc[df[\"y_kWh\"] > 0, \"y_kWh\"].mean()\n",
    "\n",
    "    df[\"y_kWh_fixed\"].fillna(global_mean, inplace=True)\n",
    "\n",
    "    # Nettoyage colonnes internes\n",
    "    df.drop(columns=[\"y_kWh_interpolated\"], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4d150f",
   "metadata": {},
   "source": [
    "Appliquons cette fonction sur notre data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3377de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fix_energy_gaps(df)\n",
    "print(df.loc[indices, [\"y_kWh\", \"y_kWh_fixed\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remplacer les valeurs y_kWh = 0 par les valeurs y_kWh_fixed\n",
    "df.loc[df[\"y_kWh\"] == 0, \"y_kWh\"] = df.loc[df[\"y_kWh\"] == 0, \"y_kWh_fixed\"]\n",
    "\n",
    "# Supprimer la colonne y_kWh_fixed\n",
    "#df.drop(columns=[\"y_kWh_fixed\"], inplace=True)\n",
    "df.drop(columns=[\"y_kWh_fixed\",\"is_gap\", \"is_encadré\", \"problematic_gap\"], inplace=True)\n",
    "\n",
    "# Vérification\n",
    "print(df.loc[indices, \"y_kWh\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8436c0b4",
   "metadata": {},
   "source": [
    "Enregistrement des nouvelles valeurs dans le fichier csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4918e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"preprocessing_results_part2/APT1_features_filled.csv\"\n",
    "df.to_csv(csv_file, index=False)\n",
    "print(f\"Fichier {csv_file} mis à jour avec les valeurs corrigées de y_kWh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b9bb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation des changements\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.plot(df['timestamp'], df['y_kWh'], marker='o', linestyle='-')\n",
    "plt.title(\"Variation de la consommation horaire (y_kWh) dans le temps\")\n",
    "plt.xlabel(\"Timestamp\")\n",
    "plt.ylabel(\"y_kWh\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5f8b7",
   "metadata": {},
   "source": [
    "### Refaire la même procedure pour tous les autres fichiers des deux dossiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49980b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FILES = 12\n",
    "\n",
    "for i in range(2, N_FILES + 1):\n",
    "    print(f\"Traitement du {i}ème bâtiment\")\n",
    "    # lecture du fichier csv des données\n",
    "    df = pd.read_csv(f\"preprocessing_results/APT{i}_processed.csv\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if nb_zeros != 0:\n",
    "        df = fix_energy_gaps(df)\n",
    "        \n",
    "        # Remplacer les valeurs y_kWh = 0 par les valeurs y_kWh_fixed\n",
    "        df.loc[df[\"y_kWh\"] == 0, \"y_kWh\"] = df.loc[df[\"y_kWh\"] == 0, \"y_kWh_fixed\"]\n",
    "\n",
    "        # Supprimer la colonne y_kWh_fixed\n",
    "        df.drop(columns=[\"y_kWh_fixed\", \"is_gap\", \"is_encadré\", \"problematic_gap\"], inplace=True)\n",
    "\n",
    "    # Enregistrement des modifications\n",
    "    csv_file = f\"preprocessing_results_part2/APT{i}_features_filled.csv\"\n",
    "    df.to_csv(csv_file, index=False)\n",
    "    nb_zeros = (df[\"y_kWh\"] == 0).sum()\n",
    "    print(\"Nombre de lignes où y_kWh = 0 après traitement:\", nb_zeros)\n",
    "    print(f\"Fichier {csv_file} mis à jour avec les valeurs corrigées de y_kWh.\")\n",
    "        \n",
    "    # visualisation des changements\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.plot(df['timestamp'], df['y_kWh'], marker='o', linestyle='-')\n",
    "    plt.title(f\"Variation de la consommation horaire du {i}ème dans le temps\")\n",
    "    plt.xlabel(\"Timestamp\")\n",
    "    plt.ylabel(\"y_kWh\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b072f",
   "metadata": {},
   "source": [
    "#### Dans ce code en dessous, nous allons juste regrouper le résultat de prétraitement de données dans une seule dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bee5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Récupère tous les fichiers CSV dans le dossier\n",
    "chemin_fichiers = \"preprocessing_results_part2/*.csv\"\n",
    "liste_fichiers = glob.glob(chemin_fichiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "import re\n",
    "for fichier in liste_fichiers:\n",
    "    df = pd.read_csv(fichier)\n",
    "    \n",
    "    nom_fichier = os.path.basename(fichier)  # ex: 'APT9_features_filled.csv'\n",
    "    match = re.search(r'APT(\\d+)_', nom_fichier)\n",
    "    if match:\n",
    "        building_id = int(match.group(1))\n",
    "    else:\n",
    "        building_id = None  # ou une valeur par défaut si besoin\n",
    "    \n",
    "    df['building_id'] = building_id\n",
    "    \n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda88cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concaténation\n",
    "df_total = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c04cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_total.shape)\n",
    "print(df_total.head())\n",
    "print(df_total['building_id'].unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a381ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0405098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sauvegarde des données finales\n",
    "csv_file = \"final_dataset/data.csv\"\n",
    "df_total.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d7123a",
   "metadata": {},
   "source": [
    "Dans le code suivant, nous allons afficher la variation de la cible y_Kwh en fonction des  autres features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb5b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('final_dataset/data.csv')\n",
    "\n",
    "# Liste des features à analyser (hors timestamp et y_kWh)\n",
    "features = [\n",
    "    'year', 'month', 'day_of_week', 'hour', 'is_weekend', \n",
    "    'n_occupants', 'building_type', 'ownership', \n",
    "    'is_peak'\n",
    "]\n",
    "\n",
    "# Création d'une grande figure avec une grille 3×3 (9 features)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()  # Facilite l'accès aux sous-plots\n",
    "\n",
    "for idx, feature in enumerate(features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Numériques\n",
    "    if df[feature].dtype in ['int64', 'float64']:\n",
    "        sns.scatterplot(\n",
    "            x=df[feature], y=df['y_kWh'], \n",
    "            hue=df['building_id'], palette='tab10', alpha=0.4, ax=ax, legend=False\n",
    "        )\n",
    "        sns.lineplot(\n",
    "            x=df[feature],\n",
    "            y=df['y_kWh'].rolling(50, min_periods=1).mean(),\n",
    "            color='black', ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"{feature} vs y_kWh (numérique)\")\n",
    "    \n",
    "    # Catégorielles\n",
    "    else:\n",
    "        sns.boxplot(\n",
    "            x=df[feature], y=df['y_kWh'], \n",
    "            hue=df['building_id'], palette='tab10', ax=ax\n",
    "        )\n",
    "        ax.set_title(f\"{feature} (catégorielle)\")\n",
    "        ax.legend().remove()  # Pour éviter trop de légendes répétées\n",
    "    \n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel(\"y_kWh\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.4)\n",
    "\n",
    "# Une seule légende globale pour les buildings\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, title='Building ID', loc='upper right')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.95, 1])  # Légende à droite\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2677aaa1",
   "metadata": {},
   "source": [
    "On va maintenant mesurer à quel point chaque feature a une relation linéaire ou non linéaire avec ta cible y_kWh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e39f658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def analyse_linearite(df, target='y_kWh'):\n",
    "    results = []\n",
    "\n",
    "    # Liste features numériques seulement\n",
    "    numeric_features = df.select_dtypes(include=['int64','float64']).columns\n",
    "    numeric_features = [f for f in numeric_features if f != target]\n",
    "\n",
    "    for feature in numeric_features:\n",
    "        # Drop NA\n",
    "        sub = df[[feature, target]].dropna()\n",
    "        if len(sub) < 30:\n",
    "            continue\n",
    "\n",
    "        X = sub[[feature]].values\n",
    "        y = sub[target].values\n",
    "\n",
    "        # ---- Régression linéaire ----\n",
    "        lin_model = LinearRegression()\n",
    "        lin_model.fit(X, y)\n",
    "        y_pred_lin = lin_model.predict(X)\n",
    "        r2_lin = r2_score(y, y_pred_lin)\n",
    "\n",
    "        # Corrélation de Pearson\n",
    "        corr = sub[[feature, target]].corr().iloc[0,1]\n",
    "\n",
    "        # ---- Modèle non-linéaire ----\n",
    "        rf = RandomForestRegressor(n_estimators=80, random_state=0)\n",
    "        rf.fit(X, y)\n",
    "        y_pred_rf = rf.predict(X)\n",
    "        r2_rf = r2_score(y, y_pred_rf)\n",
    "\n",
    "        # Importance de non-linéarité\n",
    "        nonlinear_gain = r2_rf - r2_lin\n",
    "\n",
    "        results.append({\n",
    "            \"feature\": feature,\n",
    "            \"corr_pearson\": corr,\n",
    "            \"R²_lin\": r2_lin,\n",
    "            \"R²_random_forest\": r2_rf,\n",
    "            \"gain_non_lineaire\": nonlinear_gain\n",
    "        })\n",
    "\n",
    "    # Résultats triés selon importance de non-linéarité, Plus ce gain est grand, plus la non-linéarité est importante.\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values(by=\"gain_non_lineaire\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0ce36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# on visualise les résultats\n",
    "results = analyse_linearite(df)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c3cdc7",
   "metadata": {},
   "source": [
    "## Entrainement de modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1a5e20",
   "metadata": {},
   "source": [
    "D'après ce qu'on a fait avant, on constate que :\n",
    "- building_id n’est pas vraiment une feature informative pour la consommation.\n",
    "\n",
    "    - La valeur en elle-même (1, 2, 3…) n’a pas de lien physique avec y_kWh.\n",
    "\n",
    "    - Les patterns que ton modèle pourrait “apprendre” seraient juste des idiosyncrasies propres à chaque bâtiment, pas des relations généralisables.\n",
    "- ownership n'est pas un feature intéressant pour l'entraînement :\n",
    "    - 10 bâtiments “rented”, 1 “bought”, 1 “owned”.\n",
    "\n",
    "    - La catégorie “rented” domine largement.\n",
    "\n",
    "    - Les modèles vont très probablement apprendre à prédire “rented” tout le temps, et donc l’effet des autres catégories sera quasiment invisible. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d0aa45",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1f3e0f",
   "metadata": {},
   "source": [
    "Nous allons commencer par entraîner et comparer 3 modèles baseline de régression pour prédire la consommation énergétique (y_kWh) :\n",
    "\n",
    "- Ridge Regression (linéaire régularisé)\n",
    "- Random Forest (ensemble non-linéaire)\n",
    "- LightGBM (gradient boosting avancé)\n",
    "\n",
    "Ce bloc calcule (ou recalcule) R² et RMSE pour les modèles entrainés dans ce notebook : Ridge, Le résultat est affiché sous forme d'un tableau pandas (DataFrame) pour faciliter la comparaison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776fa61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul et tableau récapitulatif des métriques (R² & RMSE)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Re-construction des splits si nécessaire\n",
    "try:\n",
    "    X_train, X_test, y_train, y_test\n",
    "except NameError:\n",
    "    df = pd.read_csv(\"./final_dataset/data.csv\", parse_dates=[\"timestamp\"])\n",
    "    features = [\n",
    "        \"n_occupants\",\n",
    "        \"hour\",\n",
    "        \"hour_sin\",\n",
    "        \"hour_cos\",\n",
    "        \"day_of_week\",\n",
    "        \"dow_sin\",\n",
    "        \"dow_cos\",\n",
    "        \"month\",\n",
    "        \"month_sin\",\n",
    "        \"month_cos\",\n",
    "        \"is_weekend\",\n",
    "        \"is_peak\"\n",
    "    ]\n",
    "    target = \"y_kWh\"\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "metrics = []\n",
    "\n",
    "# 1) Ridge (use scaled features)\n",
    "try:\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_train_scaled, y_train)\n",
    "    pred_ridge = ridge.predict(X_test_scaled)\n",
    "    r2_ridge = r2_score(y_test, pred_ridge)\n",
    "    rmse_ridge = np.sqrt(mean_squared_error(y_test, pred_ridge))\n",
    "    metrics.append({\"model\": \"Ridge\", \"R2\": r2_ridge, \"RMSE\": rmse_ridge})\n",
    "except Exception as e:\n",
    "    print(\"Erreur Ridge:\", e)\n",
    "\n",
    "# 2) Random Forest\n",
    "try:\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    pred_rf = rf.predict(X_test)\n",
    "    r2_rf = r2_score(y_test, pred_rf)\n",
    "    rmse_rf = np.sqrt(mean_squared_error(y_test, pred_rf))\n",
    "    metrics.append({\"model\": \"RandomForest\", \"R2\": r2_rf, \"RMSE\": rmse_rf})\n",
    "except Exception as e:\n",
    "    print(\"Erreur RandomForest:\", e)\n",
    "\n",
    "# 3) LightGBM (sklearn wrapper) - fallback automatique si early_stopping not supported\n",
    "try:\n",
    "    lgbm = lgb.LGBMRegressor(objective='regression', n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=42)\n",
    "    lgbm.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=False\n",
    "    )\n",
    "    pred_lgb = lgbm.predict(X_test)\n",
    "    r2_lgb = r2_score(y_test, pred_lgb)\n",
    "    rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "    metrics.append({\"model\": \"LightGBM\", \"R2\": r2_lgb, \"RMSE\": rmse_lgb, \"best_iter\": getattr(lgbm, 'best_iteration_', None)})\n",
    "except TypeError:\n",
    "    # fallback si la version de LightGBM n'accepte pas early_stopping_rounds\n",
    "    try:\n",
    "        callbacks = [lgb.callback.early_stopping(50)]\n",
    "    except Exception:\n",
    "        callbacks = []\n",
    "    lgbm = lgb.LGBMRegressor(objective='regression', n_estimators=500, learning_rate=0.05, num_leaves=31, random_state=42)\n",
    "    lgbm.fit(X_train, y_train, eval_set=[(X_test, y_test)], callbacks=callbacks, verbose=False)\n",
    "    pred_lgb = lgbm.predict(X_test)\n",
    "    r2_lgb = r2_score(y_test, pred_lgb)\n",
    "    rmse_lgb = np.sqrt(mean_squared_error(y_test, pred_lgb))\n",
    "    metrics.append({\"model\": \"LightGBM\", \"R2\": r2_lgb, \"RMSE\": rmse_lgb, \"best_iter\": getattr(lgbm, 'best_iteration_', None)})\n",
    "except Exception as e:\n",
    "    print(\"Erreur LightGBM:\", e)\n",
    "\n",
    "# Résultats en tableau\n",
    "metrics_df = pd.DataFrame(metrics).round(4)\n",
    "metrics_df = metrics_df[[\"model\", \"R2\", \"RMSE\"]]\n",
    "metrics_df = metrics_df.sort_values(by=\"R2\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"Résumé des métriques (R² et RMSE) :\")\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc93108",
   "metadata": {},
   "source": [
    "#### Résultats\n",
    "| Modèle | R² Test | RMSE | Analyse |\n",
    "|--------|---------|------|---------|\n",
    "| **Ridge** | 0.1343 | 0.2655 | Baseline linéaire acceptable |\n",
    "| **Random Forest** | 0.2031 | 0.2547 | Meilleur modèle non-linéaire initial |\n",
    "| **LightGBM** | 0.2117 | 0.2534 | Légère amélioration sur RF |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70017240",
   "metadata": {},
   "source": [
    "- Les performances actuelles (R² ≈ 0.2 pour LGBM) sont conformes pour une première étape, surtout sur des séries temporelles avec beaucoup de bruit et peu de features.\n",
    "\n",
    "    - Ce n’est pas encore très performant si tu vises une prédiction précise de la consommation. R² de 0.2 signifie que ~80 % de la variance n’est pas expliquée par ton modèle actuel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28699b",
   "metadata": {},
   "source": [
    "##### Possibilités d’amélioration\n",
    "\n",
    "- Hyperparamètres : tu peux tuner les modèles d’arbres (Random Forest / LGBM) via GridSearch ou Optuna pour optimiser max_depth, num_leaves, learning_rate, etc.\n",
    "\n",
    "- Features supplémentaires : interactions entre hour, day_of_week, month, ou lissage/rolling sur les consommations précédentes peuvent aider.\n",
    "\n",
    "- Modèles séquentiels : si ton dataset est dense et régulier, des modèles comme LSTM ou Temporal Convolutional Networks (TCN) peuvent capturer les dépendances temporelles mieux que des modèles statiques.\n",
    "\n",
    "- Ensemble : combiner LGBM + RF + Ridge via moyenne pondérée ou stacking peut stabiliser et améliorer légèrement les prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171db08",
   "metadata": {},
   "source": [
    "Nous allons essayer enrechir l'ensemble des caractéristiques par plus de features dérivées tout en étudiant leur corrélation avec la variable cible y_kwh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e0a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ============================\n",
    "# Charger le dataset\n",
    "# ============================\n",
    "df = pd.read_csv('final_dataset/data.csv', parse_dates=['timestamp'])\n",
    "\n",
    "# Création de features dérivées pour capter patterns temporels\n",
    "df['hour_day_interaction'] = df['hour'] * df['day_of_week']\n",
    "df['month_day_interaction'] = df['month'] * df['day_of_week']\n",
    "df['hour_n_occupants'] = df['hour'] * df['n_occupants']\n",
    "\n",
    "# Rolling features sur consommation précédente (lag=1h)\n",
    "df['y_kWh_lag1'] = df['y_kWh'].shift(1)\n",
    "df['y_kWh_lag2'] = df['y_kWh'].shift(2)\n",
    "df = df.fillna(0)  # Remplacer NA des lags par 0 ou autre stratégie\n",
    "\n",
    "# ============================\n",
    "# Liste features à analyser (hors timestamp et y_kWh)\n",
    "# ============================\n",
    "features_to_analyse = [\n",
    "    'year',\n",
    "    'month',\n",
    "    'day_of_week',\n",
    "    'hour',\n",
    "    'is_weekend',\n",
    "    'n_occupants',\n",
    "    'is_peak',\n",
    "    'hour_day_interaction',\n",
    "    'month_day_interaction',\n",
    "    'hour_n_occupants',\n",
    "    'y_kWh_lag1',\n",
    "    'y_kWh_lag2'\n",
    "]\n",
    "\n",
    "\n",
    "def analyse_linearite(df, target='y_kWh'):\n",
    "    results = []\n",
    "    \n",
    "    # Sélection features numériques seulement\n",
    "    numeric_features = df[features_to_analyse].select_dtypes(\n",
    "        include=['int64', 'float64']\n",
    "    ).columns\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        sub = df[[feature, target]].dropna()\n",
    "        if len(sub) < 30:\n",
    "            continue\n",
    "        \n",
    "        X = sub[[feature]].values\n",
    "        y = sub[target].values\n",
    "        \n",
    "        # ---- Régression linéaire ----\n",
    "        lin_model = LinearRegression()\n",
    "        lin_model.fit(X, y)\n",
    "        y_pred_lin = lin_model.predict(X)\n",
    "        r2_lin = r2_score(y, y_pred_lin)\n",
    "        \n",
    "        # Corrélation de Pearson\n",
    "        corr = sub[[feature, target]].corr().iloc[0, 1]\n",
    "        \n",
    "        # ---- Modèle non-linéaire avec Random Forest mieux paramétré ----\n",
    "        rf = RandomForestRegressor(\n",
    "            n_estimators=200,        # plus d'arbres\n",
    "            max_depth=15,            # plus profond\n",
    "            min_samples_split=5,     # éviter overfitting\n",
    "            min_samples_leaf=3,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        rf.fit(X, y)\n",
    "        y_pred_rf = rf.predict(X)\n",
    "        r2_rf = r2_score(y, y_pred_rf)\n",
    "        \n",
    "        nonlinear_gain = r2_rf - r2_lin\n",
    "        \n",
    "        results.append({\n",
    "            \"feature\": feature,\n",
    "            \"corr_pearson\": corr,\n",
    "            \"R²_lin\": r2_lin,\n",
    "            \"R²_random_forest\": r2_rf,\n",
    "            \"gain_non_lineaire\": nonlinear_gain\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df.sort_values(by=\"gain_non_lineaire\", ascending=False)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Exécution\n",
    "# ============================\n",
    "linearite_results = analyse_linearite(df)\n",
    "print(linearite_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d894d",
   "metadata": {},
   "source": [
    "- Features avec fort gain non-linéaire (top 3) :\n",
    "\n",
    "    - y_kWh_lag2 : +0.269 → La consommation il y a 2h a une relation fortement non-linéaire avec la consommation actuelle\n",
    "    - y_kWh_lag1 : +0.222 → La consommation précédente aussi (effet mémoire complexe)\n",
    "    - hour_n_occupants : +0.050 → L'interaction heure × occupants capture mieux les patterns non-linéaires\n",
    "\n",
    "- Features linéaires pures :\n",
    "\n",
    "    - is_peak : gain = 0 → Parfaitement linéaire (classification binaire simple)\n",
    "    - year : gain ≈ 0 → Peu informatif ou purement linéaire\n",
    "\n",
    "- Features faibles :\n",
    "\n",
    "    - day_of_week, month, is_weekend : Très faibles corrélations et gains → Peu prédictifs individuellement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d71978",
   "metadata": {},
   "source": [
    "Maintenant, on va entraîner à nouveau nos 3 modèles baseline et voir l'effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e335fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# ============================\n",
    "# Charger le dataset\n",
    "# ============================\n",
    "df = pd.read_csv('final_dataset/data.csv', parse_dates=['timestamp'])\n",
    "\n",
    "\n",
    "# Création de features dérivées\n",
    "df['hour_day_interaction'] = df['hour'] * df['day_of_week']\n",
    "df['hour_n_occupants'] = df['hour'] * df['n_occupants']\n",
    "\n",
    "# Lag features\n",
    "df['y_kWh_lag1'] = df['y_kWh'].shift(1)\n",
    "df['y_kWh_lag2'] = df['y_kWh'].shift(2)\n",
    "df = df.fillna(0)\n",
    "\n",
    "# ============================\n",
    "# Features sélectionnées\n",
    "# ============================\n",
    "features = [\n",
    "    'n_occupants',\n",
    "    'hour',\n",
    "    'hour_n_occupants',\n",
    "    'y_kWh_lag1',\n",
    "    'y_kWh_lag2',\n",
    "    'is_peak'\n",
    "]\n",
    "target = 'y_kWh'\n",
    "\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "# ============================\n",
    "# Split train/test\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# Normalisation pour Ridge\n",
    "# ============================\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ============================\n",
    "# 1) Ridge Regression\n",
    "# ============================\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "y_pred_ridge = ridge_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Ridge R²:\", r2_score(y_test, y_pred_ridge))\n",
    "print(\"Ridge RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_ridge)))\n",
    "\n",
    "# ============================\n",
    "# 2) Random Forest Regressor\n",
    "# ============================\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=3,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest R²:\", r2_score(y_test, y_pred_rf))\n",
    "print(\"Random Forest RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_rf)))\n",
    "\n",
    "# ============================\n",
    "# 3) LightGBM Regressor\n",
    "# ============================\n",
    "lgb_train = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_test = lgb.Dataset(X_test, label=y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    \"objective\": \"regression\",\n",
    "    \"metric\": \"rmse\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"num_leaves\": 31,\n",
    "    \"verbose\": -1,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "lgb_model = lgb.train(\n",
    "    params,\n",
    "    lgb_train,\n",
    "    num_boost_round=500,\n",
    "    valid_sets=[lgb_test],\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "y_pred_lgb = lgb_model.predict(X_test, num_iteration=lgb_model.best_iteration)\n",
    "\n",
    "print(\"LightGBM R²:\", r2_score(y_test, y_pred_lgb))\n",
    "print(\"LightGBM RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_lgb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd95e72",
   "metadata": {},
   "source": [
    "##### Résultats \n",
    "| Modèle | R² Test | RMSE | Notes |\n",
    "|--------|---------|------|-------|\n",
    "| **Ridge (features enrichies)** | 0.4069 | 0.2198 | Amélioration majeure |\n",
    "| **Random Forest (features enrichies)** | 0.3700 | 0.2265 | Bon mais moins stable |\n",
    "| **LightGBM (features enrichies)** | 0.3677 | 0.2269 | Surpassé par Ridge |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f595cd25",
   "metadata": {},
   "source": [
    "- On remarque bien que ```(y_kWh_lag1, y_kWh_lag2)``` et des interactions ```(hour_n_occupants)``` a clairement amélioré la performance.\n",
    "- On va maintenant essayerune approche Ensemble :\n",
    "**un stacking** simple pour combiner Ridge et LightGBM (ou Random Forest) afin de tirer parti à la fois de la partie linéaire et de la capacité à capturer les non-linéarités."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796165a9",
   "metadata": {},
   "source": [
    "L'ensemble des features qu'on dispose jusqu'à maintenant :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "    'hour_day_interaction','month_day_interaction','y_kWh_lag1','y_kWh_lag2'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bbc974",
   "metadata": {},
   "source": [
    "Si on définit un split normalement comme suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67fc8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[features].values\n",
    "y = df['y_kWh'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51281881",
   "metadata": {},
   "source": [
    "On risque le `data leakage`. Le data leakage (fuite de données) survient quand des informations du test set ou du futur s'infiltrent dans l'entraînement du modèle. Cela rend le modèle suroptimiste sur les performances réelles.\n",
    "On utilise le KFold Stacking qui est la bonne pratique.\n",
    "    - KFold strict est stricte : À chaque fold, le modèle voit seulement X_tr → jamais contaminé par X_val\n",
    "    - OOF (Out-Of-Fold) predictions : Les prédictions sur la validation deviennent les features du méta-modèle\n",
    "\n",
    "Le méta-modèle n'a jamais vu les données brutes de train, alors pas de leakage entre niveau 0 et niveau 1.\n",
    "\n",
    "- Niveau 0 : Ridge, LightGBM → génèrent oof_ridge, oof_lgb \n",
    "                        ↓\n",
    "- Niveau 1 : Ridge(méta) → prend oof_ridge + oof_lgb comme input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ac6a07",
   "metadata": {},
   "source": [
    "On définit donc le split de la manière suivante :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b38f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold stacking (no leakage)\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "oof_ridge = np.zeros(len(X_train)) \n",
    "oof_lgb   = np.zeros(len(X_train)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe735d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#entraîenment des modèles de base\n",
    "ridge_models = []\n",
    "lgb_models = []\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "\n",
    "    # Ridge\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    oof_ridge[valid_idx] = ridge.predict(X_val)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "    # LightGBM\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_model = lgb.train(\n",
    "        {\"objective\": \"regression\", \"metric\": \"rmse\", \"learning_rate\": 0.05, \"num_leaves\": 31},\n",
    "        train_data,\n",
    "        num_boost_round=100\n",
    "    )\n",
    "    oof_lgb[valid_idx] = lgb_model.predict(X_val)\n",
    "    lgb_models.append(lgb_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c834ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# OOF predictions = dataset du méta-modèle (niveau 1)\n",
    "X_meta_train = np.column_stack([oof_ridge, oof_lgb])\n",
    "\n",
    "# ============================\n",
    "# Méta-modèle (Ridge)\n",
    "# ============================\n",
    "meta = Ridge(alpha=1.0)\n",
    "meta.fit(X_meta_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400536f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "\n",
    "# Moyenne des prédictions des modèles entraînés en KFold\n",
    "ridge_test_pred = np.mean([m.predict(X_test) for m in ridge_models], axis=0)\n",
    "lgb_test_pred   = np.mean([m.predict(X_test) for m in lgb_models], axis=0)\n",
    "\n",
    "X_meta_test = np.column_stack([ridge_test_pred, lgb_test_pred])\n",
    "y_pred_stack = meta.predict(X_meta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce443b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des performances\n",
    "\n",
    "print(\"Stacked R²:\", r2_score(y_test, y_pred_stack))\n",
    "print(\"Stacked RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_stack)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8218fd",
   "metadata": {},
   "source": [
    "##### Analyse des résultats obtenus\n",
    " Les chiffres qu'on a obtienu sont un signe que le stacking est proprement implémenté, mais il n’apporte pas d’amélioration par rapport au meilleur modèle seul (Ridge amélioré).\n",
    "\n",
    "- Et c’est totalement normal à ce stade :\n",
    "    - On vient d’appliquer un stacking “classique”, mais les deux modèles de base (Ridge et LightGBM) sont trop similaires dans leur comportement pour que le méta-modèle apprenne quelque chose de très différent.\n",
    "\n",
    "\n",
    "- Stacked R² ≈ 0.408 — RMSE ≈ 0.2196 est pratiquement identique à notre meilleur modèle avant stacking (Ridge R² ≈ 0.4069).\n",
    "\n",
    "    - Pas de perte → pas de fuite.\n",
    "    - Pas de gain → modèles pas encore assez complémentaires.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d79436",
   "metadata": {},
   "source": [
    "- Maintenant on va améliorer la diversité de notre stacking par l'ajout de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d40039f",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "    'hour_day_interaction','month_day_interaction','y_kWh_lag1','y_kWh_lag2'\n",
    "]\n",
    "\n",
    "X = df[features].values\n",
    "y = df['y_kWh'].values\n",
    "\n",
    "\n",
    "# Train/Test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False # pour respecter l'ordre des timestamps des séries temporelles\n",
    ")\n",
    "# Echapper le data leakage\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "oof_ridge = np.zeros(len(X_train))\n",
    "oof_lgb   = np.zeros(len(X_train))\n",
    "oof_xgb   = np.zeros(len(X_train))\n",
    "  \n",
    "ridge_models = []\n",
    "lgb_models = []\n",
    "xgb_models = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61abc3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# On entraîne à nouveau les modèles de base avec la nouvelle stratégie de stacking\n",
    "# Ridge + LightGBM (KFold OOF) + XGBoost (KFold OOF)\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # Ridge\n",
    "    # -------------------------\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    oof_ridge[valid_idx] = ridge.predict(X_val)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "    # -------------------------\n",
    "    # LightGBM\n",
    "    # -------------------------\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_model = lgb.train(\n",
    "        {\"objective\": \"regression\", \"metric\": \"rmse\",\n",
    "         \"learning_rate\": 0.05, \"num_leaves\": 31},\n",
    "        train_data,\n",
    "        num_boost_round=100\n",
    "    )\n",
    "    oof_lgb[valid_idx] = lgb_model.predict(X_val)\n",
    "    lgb_models.append(lgb_model)\n",
    "\n",
    "    # -------------------------\n",
    "    # XGBoost (NON-LINEAIRE ++)\n",
    "    # -------------------------\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    oof_xgb[valid_idx] = xgb.predict(X_val)\n",
    "    xgb_models.append(xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc60f4f",
   "metadata": {},
   "source": [
    "Hyperparamètres de XGBoost :\n",
    "\n",
    "n_estimators=200 : 200 arbres\n",
    "max_depth=6 : arbres peu profonds (stable)\n",
    "subsample=0.8 : 80% données par arbre (regularisation)\n",
    "colsample_bytree=0.8 : 80% features par arbre\n",
    "\n",
    "Raison d'ajouter XGBoost : Diversité → stacking plus puissant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351c27d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset pour méta-modèle\n",
    "\n",
    "X_meta_train = np.column_stack([oof_ridge, oof_lgb, oof_xgb])\n",
    "# Méta-modèle (Ridge simple)\n",
    "\n",
    "meta = Ridge(alpha=1.0)\n",
    "meta.fit(X_meta_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions\n",
    "\n",
    "ridge_test_pred = np.mean([m.predict(X_test) for m in ridge_models], axis=0)\n",
    "lgb_test_pred   = np.mean([m.predict(X_test) for m in lgb_models], axis=0)\n",
    "xgb_test_pred   = np.mean([m.predict(X_test) for m in xgb_models], axis=0)\n",
    "\n",
    "X_meta_test = np.column_stack([ridge_test_pred, lgb_test_pred, xgb_test_pred])\n",
    "y_pred_stack = meta.predict(X_meta_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f01e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation des performancesà nouveau\n",
    "\n",
    "print(\"Stacked R²:\", r2_score(y_test, y_pred_stack))\n",
    "print(\"Stacked RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_stack)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364559b7",
   "metadata": {},
   "source": [
    "On remarque que le R² =  0.3945, ce qui montre une dégradation (-0.0124), cela est peut être dû à la forte corrélation entre les modèles.\n",
    "\n",
    "On va donc essayer de :\n",
    "\n",
    "- Augmenter la régularisation dans le stacking en remplaçant le méta-modèle Ridge par un ElasticNet. ElasticNet mélange la pénalité L2 (Ridge, douce et régulière) avec la L1 (Lasso, qui force certains poids à zéro). C’est comme dire au méta-modèle : « si un modèle ne sert à rien, tu as le droit de l’ignorer ».\n",
    "- Remplacer Ridge par un méta-modèle plus expressif, par exemple un petit GradientBoostingRegressor. C’est un méta-modèle non linéaire capable d’apprendre les erreurs spécifiques de chaque base learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7d24d9",
   "metadata": {},
   "source": [
    "#### Approche d'essai 1    \n",
    "- XGBoost dans les base learners\n",
    "\n",
    "- Stacking sans fuite (OOF correct)\n",
    "\n",
    "- Méta-modèle ElasticNet (régularisation L1+L2)\n",
    "\n",
    "- Préparation test/validation cohérente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac26c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "    'hour_day_interaction','month_day_interaction','y_kWh_lag1','y_kWh_lag2'\n",
    "]\n",
    "\n",
    "X = df[features].values\n",
    "y = df['y_kWh'].values\n",
    "\n",
    "# ============================\n",
    "# 3. Train/Test split\n",
    "# ============================\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# ============================\n",
    "# 4. KFold Stacking (no leakage)\n",
    "# ============================\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "oof_ridge = np.zeros(len(X_train))\n",
    "oof_lgb   = np.zeros(len(X_train))\n",
    "oof_xgb   = np.zeros(len(X_train))\n",
    "\n",
    "ridge_models = []\n",
    "lgb_models = []\n",
    "xgb_models = []\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # Ridge\n",
    "    # -------------------------\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    oof_ridge[valid_idx] = ridge.predict(X_val)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "    # -------------------------\n",
    "    # LightGBM\n",
    "    # -------------------------\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_model = lgb.train(\n",
    "        {\"objective\": \"regression\", \"metric\": \"rmse\",\n",
    "         \"learning_rate\": 0.05, \"num_leaves\": 31},\n",
    "        train_data,\n",
    "        num_boost_round=100\n",
    "    )\n",
    "    oof_lgb[valid_idx] = lgb_model.predict(X_val)\n",
    "    lgb_models.append(lgb_model)\n",
    "\n",
    "    # -------------------------\n",
    "    # XGBoost\n",
    "    # -------------------------\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=200, learning_rate=0.05,\n",
    "        max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "        objective='reg:squarederror', random_state=42\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    oof_xgb[valid_idx] = xgb.predict(X_val)\n",
    "    xgb_models.append(xgb)\n",
    "\n",
    "# ============================\n",
    "# 5. Meta dataset\n",
    "# ============================\n",
    "X_meta_train = np.column_stack([oof_ridge, oof_lgb, oof_xgb])\n",
    "\n",
    "# ============================\n",
    "# 6. Méta-modèle régularisé (ElasticNet)\n",
    "# ============================\n",
    "meta = ElasticNet(alpha=0.1, l1_ratio=0.3)\n",
    "meta.fit(X_meta_train, y_train)\n",
    "\n",
    "# ============================\n",
    "# 7. Test predictions\n",
    "# ============================\n",
    "ridge_test_pred = np.mean([m.predict(X_test) for m in ridge_models], axis=0)\n",
    "lgb_test_pred   = np.mean([m.predict(X_test) for m in lgb_models], axis=0)\n",
    "xgb_test_pred   = np.mean([m.predict(X_test) for m in xgb_models], axis=0)\n",
    "\n",
    "X_meta_test = np.column_stack([ridge_test_pred, lgb_test_pred, xgb_test_pred])\n",
    "y_pred_stack = meta.predict(X_meta_test)\n",
    "\n",
    "# ============================\n",
    "# 8. Evaluation\n",
    "# ============================\n",
    "print(\"Stacked R²:\", r2_score(y_test, y_pred_stack))\n",
    "print(\"Stacked RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_stack)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31ac0c3",
   "metadata": {},
   "source": [
    "#### Analyse de résultat de l'approche 1 :\n",
    "Le stacking régularisé est devenu pire qu’une ligne horizontale qui prédit la moyenne. Ce résultat est typique quand :\n",
    "\n",
    "Le méta-modèle est trop pénalisé (ElasticNet a trop écrasé les poids).\n",
    "\n",
    "Les base learners ont des prédictions corrélées ⇒ ElasticNet a annulé l’information utile.\n",
    "\n",
    "Les OOF du stack ne sont pas bien calibrés.\n",
    "\n",
    "L1 pénalise trop dans un stack avec 3 colonnes seulement.\n",
    "\n",
    "Bref : l’ElasticNet n’a pas aimé ce dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6545994b",
   "metadata": {},
   "source": [
    "### Approche d'essai 2 \n",
    "- XGBoost dans les base learners\n",
    "\n",
    "- Stacking sans fuite (OOF correct)\n",
    "\n",
    "- Méta-modèle GradientBoost \n",
    "\n",
    "- Préparation test/validation cohérente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9f7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "\n",
    "oof_ridge = np.zeros(len(X_train))\n",
    "oof_lgb   = np.zeros(len(X_train))\n",
    "oof_xgb   = np.zeros(len(X_train))\n",
    "\n",
    "ridge_models = []\n",
    "lgb_models = []\n",
    "xgb_models = []\n",
    "\n",
    "for train_idx, valid_idx in kf.split(X_train):\n",
    "    X_tr, X_val = X_train[train_idx], X_train[valid_idx]\n",
    "    y_tr, y_val = y_train[train_idx], y_train[valid_idx]\n",
    "\n",
    "    # -------------------------\n",
    "    # Ridge\n",
    "    # -------------------------\n",
    "    ridge = Ridge(alpha=1.0)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    oof_ridge[valid_idx] = ridge.predict(X_val)\n",
    "    ridge_models.append(ridge)\n",
    "\n",
    "    # -------------------------\n",
    "    # LightGBM\n",
    "    # -------------------------\n",
    "    train_data = lgb.Dataset(X_tr, label=y_tr)\n",
    "    lgb_model = lgb.train(\n",
    "        {\"objective\": \"regression\", \"metric\": \"rmse\",\n",
    "         \"learning_rate\": 0.05, \"num_leaves\": 31},\n",
    "        train_data,\n",
    "        num_boost_round=100\n",
    "    )\n",
    "    oof_lgb[valid_idx] = lgb_model.predict(X_val)\n",
    "    lgb_models.append(lgb_model)\n",
    "\n",
    "    # -------------------------\n",
    "    # XGBoost\n",
    "    # -------------------------\n",
    "    xgb = XGBRegressor(\n",
    "        n_estimators=200, learning_rate=0.05,\n",
    "        max_depth=6, subsample=0.8, colsample_bytree=0.8,\n",
    "        objective='reg:squarederror', random_state=42\n",
    "    )\n",
    "    xgb.fit(X_tr, y_tr)\n",
    "    oof_xgb[valid_idx] = xgb.predict(X_val)\n",
    "    xgb_models.append(xgb)\n",
    "\n",
    "# ============================\n",
    "# 5. Meta dataset\n",
    "# ============================\n",
    "X_meta_train = np.column_stack([oof_ridge, oof_lgb, oof_xgb])\n",
    "\n",
    "# ============================\n",
    "# 6. Méta-modèle régularisé (ElasticNet)\n",
    "# ============================\n",
    "meta = GradientBoostingRegressor(\n",
    "    n_estimators=200,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=3,\n",
    "    subsample=0.8\n",
    ")\n",
    "meta.fit(X_meta_train, y_train)\n",
    "\n",
    "# ============================\n",
    "# 7. Test predictions\n",
    "# ============================\n",
    "ridge_test_pred = np.mean([m.predict(X_test) for m in ridge_models], axis=0)\n",
    "lgb_test_pred   = np.mean([m.predict(X_test) for m in lgb_models], axis=0)\n",
    "xgb_test_pred   = np.mean([m.predict(X_test) for m in xgb_models], axis=0)\n",
    "\n",
    "X_meta_test = np.column_stack([ridge_test_pred, lgb_test_pred, xgb_test_pred])\n",
    "y_pred_stack = meta.predict(X_meta_test)\n",
    "\n",
    "# ============================\n",
    "# 8. Evaluation\n",
    "# ============================\n",
    "print(\"Stacked R²:\", r2_score(y_test, y_pred_stack))\n",
    "print(\"Stacked RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_stack)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ee52e",
   "metadata": {},
   "source": [
    "Il fait même légèrement moins bien que ton Ridge (≈0.40 R²) et ton stacking initial (~0.41 R²).\n",
    "\n",
    "C’est un signal très instructif :\n",
    "ton ensemble de modèles ne contient probablement plus d’information nouvelle à combiner.\n",
    "\n",
    "En clair :\n",
    "ton Ridge + LightGBM + XGBoost donnent tous à peu près la même structure prédictive.\n",
    "Donc un méta-modèle (même non linéaire) ne peut pas exploiter une différence informative → il n’y a rien à \"empiler\" pour améliorer.\n",
    "\n",
    "- Ce comportement est normal quand :\n",
    "\n",
    "    - les features sont plutôt simples,\n",
    "\n",
    "    - les modèles capturent déjà le maximum de signal,\n",
    "\n",
    "    - les lags dominent la prédiction, et la variabilité restante est du bruit.\n",
    "- On a R² maxi actuel ≈ 0.41 (≈ 41% de variance expliquée)\n",
    "\n",
    "- Ce plafond suggère que :\n",
    "\n",
    "    - soit notre dataset manque de features explicatives,\n",
    "    - soit une partie importante de la consommation est intrinsèquement imprévisible,\n",
    "    - soit les transformations actuelles n’exposent pas bien certaines relations temporelles.\n",
    "\n",
    "- Ce qu'il faut faire dans ce cas :\n",
    "    - Ajouter des features temporelles plus riches\n",
    "    - Ajouter des features de périodicité\n",
    "    - On peut penser à un modèle séquentiel si ça va marcher.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb2ecc",
   "metadata": {},
   "source": [
    "Nous allons essayer d'ajouter plus des features et étudier leurs corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca4113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de features dérivées pour capter patterns temporels\n",
    "\n",
    "df['hour_day_interaction'] = df['hour'] * df['day_of_week']\n",
    "df['month_day_interaction'] = df['month'] * df['day_of_week']\n",
    "df['y_kWh_lag1'] = df['y_kWh'].shift(1)\n",
    "df['y_kWh_lag2'] = df['y_kWh'].shift(2)\n",
    "\n",
    "# Lags longs\n",
    "df[\"lag_24\"] = df[\"y_kWh\"].shift(24)\n",
    "df[\"lag_48\"] = df[\"y_kWh\"].shift(48)\n",
    "df[\"lag_168\"] = df[\"y_kWh\"].shift(168)\n",
    "\n",
    "# Rolling means\n",
    "df[\"rolling_6h\"] = df[\"y_kWh\"].rolling(6).mean()\n",
    "df[\"rolling_24h\"] = df[\"y_kWh\"].rolling(24).mean()\n",
    "df[\"rolling_7d\"] = df[\"y_kWh\"].rolling(168).mean()\n",
    "\n",
    "# Rolling volatility\n",
    "df[\"std_24h\"] = df[\"y_kWh\"].rolling(24).std()\n",
    "\n",
    "# Expanding features\n",
    "df[\"expanding_mean\"] = df[\"y_kWh\"].expanding().mean()\n",
    "df[\"expanding_std\"] = df[\"y_kWh\"].expanding().std()\n",
    "\n",
    "df = df.fillna(0)\n",
    "\n",
    "features = [\n",
    "    'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "    'hour_day_interaction','month_day_interaction',\n",
    "    'y_kWh_lag1','y_kWh_lag2','lag_24','lag_48','lag_168',\n",
    "    'rolling_6h','rolling_24h','rolling_7d',\n",
    "    'std_24h','expanding_mean','expanding_std'\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126db559",
   "metadata": {},
   "source": [
    "On étudie la corrélation de ces nouvelles features avec la variable cible :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b2ec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====== Features avancées ajoutées ======\n",
    "advanced_features = [\n",
    "    'hour_day_interaction',\n",
    "    'month_day_interaction',\n",
    "    'y_kWh_lag1',\n",
    "    'y_kWh_lag2',\n",
    "    'lag_24',\n",
    "    'lag_48',\n",
    "    'lag_168',\n",
    "    'lag_336',\n",
    "    'rolling_6h',\n",
    "    'rolling_24h',\n",
    "    'rolling_7d',\n",
    "    'std_24h',\n",
    "    'expanding_mean',\n",
    "    'expanding_std',\n",
    "    'diff_1h',\n",
    "    'diff_24h',\n",
    "    'rolling_24h_pct_change',\n",
    "    'hour_n_occupants',\n",
    "    'dow_weekend'\n",
    "]\n",
    "\n",
    "# Calculer la corrélation avec la cible\n",
    "correlations = []\n",
    "for feature in advanced_features:\n",
    "    corr = df[feature].corr(df['y_kWh'])\n",
    "    correlations.append({\n",
    "        'Feature Avancée': feature,\n",
    "        'Corrélation avec y_kWh': corr\n",
    "    })\n",
    "\n",
    "# Créer un DataFrame et trier par corrélation décroissante\n",
    "df_corr = pd.DataFrame(correlations)\n",
    "df_corr = df_corr.sort_values('Corrélation avec y_kWh', ascending=False, key=abs)\n",
    "\n",
    "# Afficher le résultat\n",
    "print(\"=\" * 60)\n",
    "print(\"Corrélation des Features Avancées avec la Cible (y_kWh)\")\n",
    "print(\"=\" * 60)\n",
    "print(df_corr.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5755f7da",
   "metadata": {},
   "source": [
    "Maintenant on va essayer l'approche séquetielle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "SEQ_LEN = 168  # 1 semaine\n",
    "\n",
    "# Features enrichies\n",
    "features += ['hour_sin','hour_cos','dow_sin','dow_cos','month_sin','month_cos']\n",
    "\n",
    "# Normalisation\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(df[features].fillna(0))\n",
    "\n",
    "# Séquences\n",
    "def build_sequences(X, y, seq_len=SEQ_LEN):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(seq_len, len(X)):\n",
    "        X_seq.append(X[i-seq_len:i])\n",
    "        y_seq.append(y[i])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "y_values = df['y_kWh'].values\n",
    "X_seq, y_seq = build_sequences(X_scaled, y_values, SEQ_LEN)\n",
    "\n",
    "split = int(len(X_seq) * 0.8)\n",
    "X_train, X_test = X_seq[:split], X_seq[split:]\n",
    "y_train, y_test = y_seq[:split], y_seq[split:]\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    LSTM(128, return_sequences=True, input_shape=(SEQ_LEN, X_train.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dropout(0.2),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Évaluation\n",
    "y_pred = model.predict(X_test)\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(\"LSTM R²:\", r2_score(y_test, y_pred))\n",
    "print(\"LSTM RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a6de3",
   "metadata": {},
   "source": [
    "##### Résultat d'approche séquentielle\n",
    "R² = 0.044 → ça veut dire que notre modèle explique 4 % de la variance, autrement dit presque rien, c'est catastrophique les gars...\n",
    "\n",
    "Un R² proche de 0 signifie : « ce modèle ne capture presque aucune structure dans les données ». \n",
    "On déduit que le LSTM se trompe beaucoup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaea79e",
   "metadata": {},
   "source": [
    "Aucune des statégies de stacking n'a donnée une amélioration de performance. Puisqu'on a remarqué que l'enrechissement par les features temporelles avancées, les lag et les moyennes glissantes ont montré une forte corrélation avec la variable cible y_kwh, et le meilleur R² était presque 0.40 avec Ridge initialement sont ajout des features avancées, on a décidé qu'on va migrer vers une approche Ridge avec sélection intelligente des features lesplus significatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3fd9dd",
   "metadata": {},
   "source": [
    "### Comparaison entre Ridge et Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eabad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Charger le dataset\n",
    "\n",
    "df = pd.read_csv('final_dataset/data.csv')\n",
    "\n",
    "# ====== Features temporelles avancées ====== donnes\n",
    "\n",
    "df['hour_day_interaction'] = df['hour'] * df['day_of_week']\n",
    "df['month_day_interaction'] = df['month'] * df['day_of_week']\n",
    "df['y_kWh_lag1'] = df['y_kWh'].shift(1)\n",
    "df['y_kWh_lag2'] = df['y_kWh'].shift(2)\n",
    "df['lag_24'] = df['y_kWh'].shift(24)\n",
    "df['lag_48'] = df['y_kWh'].shift(48)\n",
    "df['lag_168'] = df['y_kWh'].shift(168)\n",
    "df['lag_336'] = df['y_kWh'].shift(336)\n",
    "df['rolling_6h'] = df['y_kWh'].shift(1).rolling(6).mean()\n",
    "df['rolling_24h'] = df['y_kWh'].shift(1).rolling(24).mean()\n",
    "df['rolling_7d'] = df['y_kWh'].shift(1).rolling(168).mean()\n",
    "df['std_24h'] = df['y_kWh'].shift(1).rolling(24).std()\n",
    "df['expanding_mean'] = df['y_kWh'].shift(1).expanding().mean()\n",
    "df['expanding_std'] = df['y_kWh'].shift(1).expanding().std()\n",
    "df['diff_1h'] = df['y_kWh'] - df['y_kWh_lag1']\n",
    "df['diff_24h'] = df['y_kWh'] - df['lag_24']\n",
    "df['rolling_24h_pct_change'] = (df['y_kWh'].shift(1) - df['rolling_24h']) / df['rolling_24h'].replace(0,1)\n",
    "df['hour_n_occupants'] = df['hour'] * df['n_occupants']\n",
    "df['dow_weekend'] = df['day_of_week'] * df['is_weekend']\n",
    "df = df.fillna(0)\n",
    "\n",
    "# Liste complète des features\n",
    "\n",
    "features = [\n",
    "'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "'hour_day_interaction','month_day_interaction',\n",
    "'y_kWh_lag1','y_kWh_lag2','lag_24','lag_48','lag_168','lag_336',\n",
    "'rolling_6h','rolling_24h','rolling_7d',\n",
    "'std_24h','expanding_mean','expanding_std',\n",
    "'diff_1h','diff_24h','rolling_24h_pct_change',\n",
    "'hour_n_occupants','dow_weekend',\n",
    "'hour_sin','hour_cos','dow_sin','dow_cos','month_sin','month_cos'\n",
    "]\n",
    "\n",
    "# Split train/test\n",
    "\n",
    "X = df[features].values\n",
    "y = df['y_kWh'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalisation\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ===== Dictionnaire pour stocker les résultats =====\n",
    "\n",
    "results = []\n",
    "\n",
    "# Fonction pour entraîner un modèle et récupérer les métriques\n",
    "\n",
    "def evaluate_model(model, param_grid, X_train, y_train, X_test, y_test, model_name, use_rfe=False, n_features=20):\n",
    "    grid = GridSearchCV(model, param_grid=param_grid, cv=5, scoring='r2')\n",
    "    grid.fit(X_train, y_train)\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "\n",
    "    if use_rfe:\n",
    "        selector = RFE(best_model, n_features_to_select=n_features)\n",
    "        selector.fit(X_train, y_train)\n",
    "        X_train_sel = selector.transform(X_train)\n",
    "        X_test_sel = selector.transform(X_test)\n",
    "        best_model.fit(X_train_sel, y_train)\n",
    "        y_pred = best_model.predict(X_test_sel)\n",
    "    else:\n",
    "        y_pred = best_model.predict(X_test)\n",
    "\n",
    "    \n",
    "    # Calcul métriques\n",
    "    r2_cv = grid.best_score_\n",
    "    r2_test = r2_score(y_test, y_pred) \n",
    "    r2_cv = grid.best_score_ \n",
    "    r2_test = r2_score(y_test, y_pred) \n",
    "    mae = mean_absolute_error(y_test, y_pred) \n",
    "    mse = mean_squared_error(y_test, y_pred) \n",
    "    rmse = np.sqrt(mse) \n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.where(y_test==0,1,y_test))) * 100\n",
    "    \n",
    "    results.append({ \n",
    "                    'Modèle': model_name + (' + RFE' if use_rfe else ''),\n",
    "                    'Meilleur alpha': grid.best_params_.get('alpha', '-'),\n",
    "                    'Meilleur l1_ratio': grid.best_params_.get('l1_ratio', '-'),\n",
    "                    'R² CV': r2_cv, 'R² test': r2_test,\n",
    "                    'MAE': mae,\n",
    "                    'MSE': mse,\n",
    "                    'RMSE': rmse,\n",
    "                    'MAPE (%)': mape })\n",
    "\n",
    "\n",
    "# ===== Évaluation Ridge + RFE =====\n",
    "\n",
    "ridge_params = {'alpha':[0.01, 0.1, 1, 10, 100]}\n",
    "evaluate_model(Ridge(), ridge_params, X_train, y_train, X_test, y_test, 'Ridge', use_rfe=True, n_features=20)\n",
    "\n",
    "# ===== Évaluation ElasticNet + RFE =====\n",
    "\n",
    "enet_params = {'alpha':[0.01, 0.1, 1, 10], 'l1_ratio':[0.1, 0.5, 0.9]}\n",
    "evaluate_model(ElasticNet(max_iter=10000), enet_params, X_train, y_train, X_test, y_test, 'ElasticNet', use_rfe=True, n_features=20)\n",
    "\n",
    "# Créer un DataFrame récapitulatif\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "print(df_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0770cc4",
   "metadata": {},
   "source": [
    "#### Résultats\n",
    "\n",
    "R² très proches de 1 → les modèles expliquent presque parfaitement la variance de la consommation (y_kWh).\n",
    "\n",
    "MAE et RMSE extrêmement faibles → erreurs absolues très petites (Ridge est quasiment parfait, erreur moyenne ~0.0002 kWh).\n",
    "\n",
    "MAPE très faible → les prédictions sont précises en pourcentage (<2 %)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0326d0",
   "metadata": {},
   "source": [
    "Maintenant on va appliquer une sélection des meilleurs 20 features à travers RFE, sur lequelles on fait l'entraîenement :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb37bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "'year','month','day_of_week','hour','is_weekend','n_occupants','is_peak',\n",
    "'hour_day_interaction','month_day_interaction',\n",
    "'y_kWh_lag1','y_kWh_lag2','lag_24','lag_48','lag_168','lag_336',\n",
    "'rolling_6h','rolling_24h','rolling_7d',\n",
    "'std_24h','expanding_mean','expanding_std',\n",
    "'diff_1h','diff_24h','rolling_24h_pct_change',\n",
    "'hour_n_occupants','dow_weekend',\n",
    "'hour_sin','hour_cos','dow_sin','dow_cos','month_sin','month_cos'\n",
    "]\n",
    "\n",
    "# Split train/test\n",
    "\n",
    "X = df[features].values\n",
    "y = df['y_kWh'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Normalisation\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# ===== Ridge + RFE =====\n",
    "\n",
    "ridge = Ridge(alpha=0.01)\n",
    "rfe_ridge = RFE(ridge, n_features_to_select=20)\n",
    "rfe_ridge.fit(X_train, y_train)\n",
    "ridge_selected_features = [f for f, s in zip(features, rfe_ridge.support_) if s]\n",
    "print(\"Ridge + RFE - Features sélectionnées :\")\n",
    "print(ridge_selected_features)\n",
    "\n",
    "# ===== ElasticNet + RFE =====\n",
    "\n",
    "enet = ElasticNet(alpha=0.01, l1_ratio=0.1, max_iter=10000)\n",
    "rfe_enet = RFE(enet, n_features_to_select=20)\n",
    "rfe_enet.fit(X_train, y_train)\n",
    "enet_selected_features = [f for f, s in zip(features, rfe_enet.support_) if s]\n",
    "print(\"\\nElasticNet + RFE - Features sélectionnées :\")\n",
    "print(enet_selected_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2a703",
   "metadata": {},
   "source": [
    "Le meilleur modèle global est Ridge + RFE, car :\n",
    "\n",
    "- Il a la plus haute performance sur le test (R² ≈ 0.999999).\n",
    "\n",
    "- Il est régularisé (Ridge) et utilise une sélection de 20 features, ce qui simplifie le modèle sans perte de précision.\n",
    "\n",
    "- ElasticNet est plus orienté vers la sparsité (L1) et n’apporte pas d’avantage ici.\n",
    "\n",
    "Les résultats confirment que les variables temporelles avancées et les lags importants (y_kWh_lag1, lag_24, lag_336) sont critiques pour la prédiction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194fc011",
   "metadata": {},
   "source": [
    "### Détection des heures de pic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36115897",
   "metadata": {},
   "source": [
    "Pour détecter les heures de pic de consommation énergétique, nous allons utiliser la fonction `.find_peaks()` de `scipy.signal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4954038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c49ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#On charge notre meilleur modèle\n",
    "\n",
    "best_ridge = Ridge(alpha=0.01)  # notre alpha optimal\n",
    "selector = RFE(best_ridge, n_features_to_select=20)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "#On effectue le split\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "best_ridge.fit(X_train_sel, y_train)\n",
    "y_pred = best_ridge.predict(X_test_sel)\n",
    "\n",
    "# y_pred : prédictions du modèle\n",
    "y_range = y_pred.max() - y_pred.min()\n",
    "peaks, _ = find_peaks(y_pred, prominence=y_range*0.1) # régler prominence selon sensibilité\n",
    "y_peak = np.zeros_like(y_pred, dtype=int)\n",
    "y_peak[peaks] = 1\n",
    "\n",
    "# Vérifier extrémités s'elles contiennent des pics\n",
    "if y[0] > y[1]:\n",
    "    peaks = np.insert(peaks, 0, 0)\n",
    "if y[-1] > y[-2]:\n",
    "    peaks = np.append(peaks, len(y)-1)\n",
    "\n",
    "# Ajouter dans un DataFrame pour analyse\n",
    "\n",
    "df_peaks = pd.DataFrame({\n",
    "'y_true': y_test,\n",
    "'y_pred': y_pred,\n",
    "'peak_pred': y_peak\n",
    "})\n",
    "print(df_peaks.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c60acc0",
   "metadata": {},
   "source": [
    "### Construire les données finales qui seront stockées et exportées pour l'application Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88289e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.feature_selection import RFE\n",
    "import pickle\n",
    "\n",
    "# Entraîner RFE + Ridge\n",
    "best_ridge = Ridge(alpha=0.01)\n",
    "selector = RFE(best_ridge, n_features_to_select=20)\n",
    "selector.fit(X_train, y_train)\n",
    "\n",
    "# Transformer les données\n",
    "X_train_sel = selector.transform(X_train)\n",
    "X_test_sel = selector.transform(X_test)\n",
    "best_ridge.fit(X_train_sel, y_train)\n",
    "y_pred = best_ridge.predict(X_test_sel)\n",
    "\n",
    "# Construire la liste des 20 features sélectionnées\n",
    "ridge_selected_features = [f for f, s in zip(features, selector.support_) if s]\n",
    "selected_features = ridge_selected_features\n",
    "\n",
    "# Sauvegarder les objets pour qu'ils soient exportés et utilisés dans l'application streamlit\n",
    "\n",
    "with open(\"scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "with open(\"selector.pkl\", \"wb\") as f:\n",
    "    pickle.dump(selector, f)\n",
    "\n",
    "with open(\"selected_features.pkl\", \"wb\") as f:\n",
    "    pickle.dump(selected_features, f)\n",
    "\n",
    "with open(\"ridge_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_ridge, f)\n",
    "\n",
    "print(\"Fichiers exportés : scaler.pkl, selector.pkl, selected_features.pkl, ridge_model.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
